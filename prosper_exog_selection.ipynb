{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prosper Survey Feature Selection\n",
    "\n",
    "Analyze all Prosper survey features (levels + MoM diffs + z-scores) using VIF to prune multicollinearity.\n",
    "\n",
    "Prosper has 36 base series across 4 question categories and 4 demographic groups, yielding 144 features with all transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Snapshot & Pivot to Wide Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path('data/Exogenous_data/prosper')\n",
    "all_snaps = sorted(base_dir.glob('**/*.parquet'))\n",
    "print(f'Total snapshots: {len(all_snaps)}')\n",
    "\n",
    "# Load 2025-12 or latest\n",
    "snap_path = base_dir / 'decades' / '2020s' / '2025' / '2025-12.parquet'\n",
    "if not snap_path.exists():\n",
    "    snap_path = all_snaps[-1]\n",
    "    print(f'Using fallback: {snap_path}')\n",
    "\n",
    "df = pd.read_parquet(snap_path)\n",
    "print(f'\\nRaw shape: {df.shape}')\n",
    "print(f'Unique series: {df[\"series_name\"].nunique()}')\n",
    "print(f'\\nAll series names:')\n",
    "for i, s in enumerate(sorted(df['series_name'].unique()), 1):\n",
    "    # Truncate for display\n",
    "    display = s[:90] + '...' if len(s) > 90 else s\n",
    "    print(f'  {i:3d}. {display}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format\n",
    "wide = df.pivot_table(index='date', columns='series_name', values='value', aggfunc='first')\n",
    "wide = wide.sort_index()\n",
    "print(f'Wide shape: {wide.shape}')\n",
    "print(f'Date range: {wide.index.min()} to {wide.index.max()}')\n",
    "print(f'\\nNaN % per feature (top 20):')\n",
    "nan_pct = wide.isna().mean().sort_values(ascending=False)\n",
    "for f, pct in nan_pct.head(20).items():\n",
    "    print(f'  {pct*100:5.1f}%  {f[:80]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. VIF Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_vif(df_wide, feature_list, group_name):\n",
    "    \"\"\"Compute VIF for a group of features.\"\"\"\n",
    "    cols = [c for c in feature_list if c in df_wide.columns]\n",
    "    if len(cols) < 2:\n",
    "        return pd.DataFrame()\n",
    "    X = df_wide[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    zero_var = X.columns[X.var() == 0]\n",
    "    if len(zero_var) > 0:\n",
    "        print(f'  Dropping {len(zero_var)} zero-variance cols')\n",
    "        X = X.loc[:, X.var() > 0]\n",
    "    if X.shape[1] < 2:\n",
    "        return pd.DataFrame()\n",
    "    X_const = X.copy()\n",
    "    X_const['_const'] = 1.0\n",
    "    vif_data = []\n",
    "    for col in [c for c in X_const.columns if c != '_const']:\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X_const.values, X_const.columns.get_loc(col))\n",
    "            vif_data.append({'group': group_name, 'feature': col, 'VIF': round(vif, 2)})\n",
    "        except Exception:\n",
    "            vif_data.append({'group': group_name, 'feature': col, 'VIF': np.nan})\n",
    "    return pd.DataFrame(vif_data).sort_values('VIF', ascending=False)\n",
    "\n",
    "\n",
    "def iterative_vif_pruning(df_wide, features, threshold=10.0, protected=None, verbose=True):\n",
    "    \"\"\"Iteratively remove highest-VIF feature until all below threshold.\"\"\"\n",
    "    protected = protected or set()\n",
    "    cols = [c for c in features if c in df_wide.columns]\n",
    "    X = df_wide[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    X = X.loc[:, X.var() > 0]\n",
    "    removed = []\n",
    "    iteration = 0\n",
    "    while len(X.columns) > 2:\n",
    "        iteration += 1\n",
    "        X_const = X.copy()\n",
    "        X_const['_const'] = 1.0\n",
    "        vifs = {}\n",
    "        for col in [c for c in X_const.columns if c != '_const']:\n",
    "            try:\n",
    "                vifs[col] = variance_inflation_factor(X_const.values, X_const.columns.get_loc(col))\n",
    "            except:\n",
    "                vifs[col] = np.inf\n",
    "        non_protected_vifs = {k: v for k, v in vifs.items() if k not in protected}\n",
    "        if not non_protected_vifs:\n",
    "            break\n",
    "        max_col = max(non_protected_vifs, key=non_protected_vifs.get)\n",
    "        max_vif = non_protected_vifs[max_col]\n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "        X = X.drop(columns=[max_col])\n",
    "        removed.append((max_col, round(max_vif, 2)))\n",
    "        if verbose and iteration <= 15:\n",
    "            print(f'  Iter {iteration}: Removed {max_col[:70]}... (VIF={max_vif:.1f})')\n",
    "        elif verbose and iteration == 16:\n",
    "            print(f'  ... (continuing)')\n",
    "    if verbose and iteration > 15:\n",
    "        print(f'  Total iterations: {iteration}')\n",
    "    return X.columns.tolist(), removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformation Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify base series (levels, not _diff or _zscore variants)\n",
    "all_series = sorted(wide.columns)\n",
    "base_series = [s for s in all_series if not s.endswith('_diff') \n",
    "               and not s.endswith('_diff_zscore_12m') \n",
    "               and not s.endswith('_diff_zscore_3m')]\n",
    "print(f'Base series: {len(base_series)}')\n",
    "\n",
    "# Compute correlation summaries\n",
    "corr_pairs = {'level_vs_diff': [], 'diff_vs_z12': [], 'diff_vs_z3': [], 'z12_vs_z3': []}\n",
    "for base in base_series:\n",
    "    diff = f'{base}_diff'\n",
    "    z12 = f'{base}_diff_zscore_12m'\n",
    "    z3 = f'{base}_diff_zscore_3m'\n",
    "    for pair_key, f1, f2 in [('level_vs_diff', base, diff), ('diff_vs_z12', diff, z12),\n",
    "                              ('diff_vs_z3', diff, z3), ('z12_vs_z3', z12, z3)]:\n",
    "        if f1 in wide.columns and f2 in wide.columns:\n",
    "            v = wide[[f1, f2]].dropna()\n",
    "            if len(v) > 10:\n",
    "                corr_pairs[pair_key].append(abs(v[f1].corr(v[f2])))\n",
    "\n",
    "print('\\nTransformation Correlation Summary:')\n",
    "print('=' * 60)\n",
    "for key, vals in corr_pairs.items():\n",
    "    if vals:\n",
    "        print(f'  {key:20s}: mean |r|={np.mean(vals):.3f}, median={np.median(vals):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Series Correlation (Same Question, Different Demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check demographic redundancy: e.g., US 18+ vs Males vs Females vs 18-34\n",
    "# For each question+answer combo, compare across demographics\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group base series by question+answer (strip demographic suffix)\n",
    "qa_groups = defaultdict(list)\n",
    "for s in base_series:\n",
    "    parts = s.rsplit(' | ', 1)\n",
    "    if len(parts) == 2:\n",
    "        qa_key = parts[0]  # question | answer\n",
    "        demo = parts[1]    # demographic group\n",
    "        qa_groups[qa_key].append((demo, s))\n",
    "\n",
    "print('Cross-Demographic Correlations (level):')\n",
    "print('=' * 70)\n",
    "for qa_key, demo_list in sorted(qa_groups.items()):\n",
    "    if len(demo_list) < 2:\n",
    "        continue\n",
    "    short_qa = qa_key[:60] + '...' if len(qa_key) > 60 else qa_key\n",
    "    print(f'\\n  {short_qa}')\n",
    "    for i in range(len(demo_list)):\n",
    "        for j in range(i+1, len(demo_list)):\n",
    "            d1, s1 = demo_list[i]\n",
    "            d2, s2 = demo_list[j]\n",
    "            if s1 in wide.columns and s2 in wide.columns:\n",
    "                v = wide[[s1, s2]].dropna()\n",
    "                if len(v) > 10:\n",
    "                    r = v[s1].corr(v[s2])\n",
    "                    flag = ' ** HIGH' if abs(r) > 0.9 else (' * mod' if abs(r) > 0.7 else '')\n",
    "                    print(f'    {d1:10s} vs {d2:10s}: r={r:+.3f}{flag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Group VIF Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(wide.columns)\n",
    "print(f'Starting with {len(all_features)} features')\n",
    "\n",
    "# Filter to 2010+ for good coverage, forward-fill\n",
    "wide_recent = wide.loc[wide.index >= '2010-01-01'].ffill()\n",
    "print(f'Using 2010+ data: {len(wide_recent)} rows')\n",
    "\n",
    "# Drop features with >30% NaN\n",
    "nan_pct = wide_recent.isna().mean()\n",
    "good_features = [f for f in all_features if nan_pct.get(f, 1.0) <= 0.3]\n",
    "bad_features = [f for f in all_features if f not in good_features]\n",
    "if bad_features:\n",
    "    print(f'Dropped {len(bad_features)} features with >30% NaN')\n",
    "    for f in bad_features:\n",
    "        print(f'  {f[:80]}  ({nan_pct[f]*100:.0f}% NaN)')\n",
    "print(f'Remaining: {len(good_features)} features')\n",
    "\n",
    "print(f'\\nIterative VIF pruning (threshold=10)...')\n",
    "final_survivors, final_removed = iterative_vif_pruning(\n",
    "    wide_recent, good_features, threshold=10.0\n",
    ")\n",
    "\n",
    "print(f'\\nFinal feature set: {len(final_survivors)} features')\n",
    "print(f'Removed: {len(final_removed)} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final VIF Verification & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FINAL RECOMMENDED FEATURE SET')\n",
    "print('=' * 60)\n",
    "print(f'Total features: {len(final_survivors)}')\n",
    "\n",
    "# Categorize\n",
    "levels, diffs, z12s, z3s = [], [], [], []\n",
    "for f in sorted(final_survivors):\n",
    "    if f.endswith('_diff_zscore_12m'): z12s.append(f)\n",
    "    elif f.endswith('_diff_zscore_3m'): z3s.append(f)\n",
    "    elif f.endswith('_diff'): diffs.append(f)\n",
    "    else: levels.append(f)\n",
    "\n",
    "print(f'\\nBy transformation:')\n",
    "print(f'  Level:           {len(levels)}')\n",
    "print(f'  MoM diff:        {len(diffs)}')\n",
    "print(f'  Diff z-score 12m: {len(z12s)}')\n",
    "print(f'  Diff z-score 3m:  {len(z3s)}')\n",
    "\n",
    "print(f'\\nLevel features:')\n",
    "for f in levels:\n",
    "    print(f'  {f[:90]}')\n",
    "print(f'\\nDiff features:')\n",
    "for f in diffs:\n",
    "    print(f'  {f[:90]}')\n",
    "print(f'\\nZ-score 12m features:')\n",
    "for f in z12s:\n",
    "    print(f'  {f[:90]}')\n",
    "print(f'\\nZ-score 3m features:')\n",
    "for f in z3s:\n",
    "    print(f'  {f[:90]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final VIF verification\n",
    "print('Final VIF Verification (should all be <= 10):')\n",
    "print('=' * 60)\n",
    "final_vif = compute_group_vif(wide_recent, final_survivors, 'final')\n",
    "if not final_vif.empty:\n",
    "    for _, row in final_vif.iterrows():\n",
    "        print(f'  VIF={row[\"VIF\"]:6.2f}  {row[\"feature\"][:80]}')\n",
    "    print(f'\\nMax VIF: {final_vif[\"VIF\"].max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the SELECTED_FEATURES set for load_prosper_data.py\n",
    "print('# Copy this into load_prosper_data.py:')\n",
    "print('SELECTED_FEATURES = {')\n",
    "for f in sorted(final_survivors):\n",
    "    print(f\"    '{f}',\")\n",
    "print('}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
